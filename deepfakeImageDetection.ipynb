{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":497301,"status":"ok","timestamp":1708790233920,"user":{"displayName":"Deepfake","userId":"14665120527405649648"},"user_tz":-330},"id":"IyJ-EBZxRdJ5"},"outputs":[],"source":["import tensorflow as tf\n","from keras import layers, models\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import os\n","import cv2\n","import numpy as np\n","\n","def load_images_from_directory(directory, label):\n","    images = []\n","    labels = []\n","    for filename in os.listdir(directory):\n","        if filename.endswith('.jpg') or filename.endswith('.png'):\n","            image_path = os.path.join(directory, filename)\n","            image = cv2.imread(image_path)\n","            image = cv2.resize(image, (150, 150))  # Resize images to a fixed size\n","            images.append(image)\n","            labels.append(label)\n","    return np.array(images), np.array(labels)\n","\n","# Define paths to your dataset directories\n","real_images_dir = '/content/drive/MyDrive/Test/Real'\n","fake_images_dir = '/content/drive/MyDrive/Test/Fake'\n","\n","# Load and preprocess real images\n","real_images, real_labels = load_images_from_directory(real_images_dir, label=0)\n","\n","# Load and preprocess fake images\n","fake_images, fake_labels = load_images_from_directory(fake_images_dir, label=1)\n","\n","# Concatenate real and fake images and shuffle them\n","all_images = np.concatenate((real_images, fake_images), axis=0)\n","all_labels = np.concatenate((real_labels, fake_labels), axis=0)\n","\n","# Shuffle data\n","shuffle_indices = np.random.permutation(len(all_images))\n","all_images = all_images[shuffle_indices]\n","all_labels = all_labels[shuffle_indices]\n","\n","# Split data into training, validation, and test sets\n","train_ratio = 0.7\n","val_ratio = 0.15\n","test_ratio = 0.15\n","\n","train_split = int(train_ratio * len(all_images))\n","val_split = int((train_ratio + val_ratio) * len(all_images))\n","\n","X_train = all_images[:train_split]\n","y_train = all_labels[:train_split]\n","\n","X_val = all_images[train_split:val_split]\n","y_val = all_labels[train_split:val_split]\n","\n","X_test = all_images[val_split:]\n","y_test = all_labels[val_split:]\n","\n","# Preprocess pixel values to range [0, 1]\n","X_train = X_train.astype('float32') / 255.0\n","X_val = X_val.astype('float32') / 255.0\n","X_test = X_test.astype('float32') / 255.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":827},"executionInfo":{"elapsed":3777120,"status":"ok","timestamp":1708794046621,"user":{"displayName":"Deepfake","userId":"14665120527405649648"},"user_tz":-330},"id":"qfAXzaQBzZae","outputId":"7260fe77-e896-4d48-b6fd-364d4ca01acd"},"outputs":[],"source":["from tensorflow.keras import models, layers\n","\n","# Define CNN model architecture #\n","model = models.Sequential([\n","    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(128, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(128, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Flatten(),\n","    layers.Dense(512, activation='relu'),\n","    layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(f'Test accuracy: {test_acc}')\n","\n","# Plot training history\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()\n","\n","# Save the trained model\n","model.save('fake_image_classifier_model.h5')\n","\n","###############################################################################################\n","# Example usage of the trained model\n","\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import numpy as np\n","import cv2\n","def preprocess_image(image_path):\n","    # Read the image using OpenCV\n","    image = cv2.imread(image_path)\n","\n","    # Resize the image to a fixed size (e.g., 150x150 pixels)\n","    image = cv2.resize(image, (150, 150))\n","\n","    # Convert the image to RGB color space (if it's in BGR format)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Normalize pixel values to range [0, 1]\n","    image = image.astype('float32') / 255.0\n","\n","    # Expand dimensions to add batch dimension\n","    image = np.expand_dims(image, axis=0)\n","\n","    return image\n","\n","def predict_image(image_path):\n","    model = tf.keras.models.load_model('fake_image_classifier_model.h5')\n","    image = preprocess_image(image_path)\n","    prediction = model.predict(image)\n","    if prediction[0][0] > 0.5:\n","        return \"Fake\"\n","    else:\n","        return \"Real\""]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n","Prediction for C:\\Users\\jjazz\\OneDrive\\Desktop\\zfake\\testfake\\testfake\\real_00002.jpg: Fake\n"]}],"source":["new_image_path = 'C:\\\\Users\\\\jjazz\\\\OneDrive\\\\Desktop\\\\zfake\\\\testfake\\\\testfake\\\\real_00002.jpg'  # Replace with the path to your new image\n","prediction = predict_image(new_image_path)\n","print(f'Prediction for {new_image_path}: {prediction}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuPQAeOYRyD1"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNpg1UVWwIbe4ky2bmx34Ny","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
